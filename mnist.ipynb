{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification in JAX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a solution to the MNIST classification problem using JAX to model and train a LeNet-300-100 feed-forward neural network for character recognition.\n",
    "\n",
    "The code is based on the [JAX](), [Haiku](), and [Optax]() example MNIST classification programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # For measuring time\n",
    "\n",
    "import numpy.random as npr  # Random number generation\n",
    "\n",
    "import jax  # Main JAX module\n",
    "import jax.numpy as jnp  # JAX's version of NumPy\n",
    "import haiku as hk  # Haiku's module for neural networks\n",
    "import optax  # Optimisers for gradient descent\n",
    "\n",
    "import datasets  # Module from JAX authors for downloading datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard set of hyperparameters are explicitly defined below along with a brief explanation of their purpose.\n",
    "A pseudo-random number generator is also initialized with a fixed seed to ensure reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2  # Learning rate for SGD.\n",
    "batch_size = 256  # Batch size for SGD (reduce if fails to fit on GPU).\n",
    "input_size = 28 * 28  # Size of the input vector to the model.\n",
    "num_epochs = 10  # Number of training epochs.\n",
    "validation_split = 0.2  # Fraction of the training data to use for validation.\n",
    "\n",
    "# Random number generator sequence.\n",
    "key_seq = hk.PRNGSequence(1729)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset module copied from the JAX example programs, the training and test data for MNIST can be loaded as follows.\n",
    "In addition, we split the training data into a training and validation set according to the aforementioned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz to ./data\n",
      "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz to ./data\n",
      "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz to ./data\n",
      "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "# Download and parse the raw MNIST dataset.\n",
    "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "\n",
    "# Calculate the number of samples in final train and validation sets\n",
    "train_samples = train_images.shape[0]\n",
    "val_samples = int(train_samples * validation_split)\n",
    "train_samples -= val_samples\n",
    "\n",
    "# Split training data into validation and final training data\n",
    "train_images, val_images = train_images[:train_samples], train_images[train_samples:]\n",
    "train_labels, val_labels = train_labels[:train_samples], train_labels[train_samples:]\n",
    "\n",
    "# Determine the number of batches in the training set\n",
    "num_complete_batches, leftover = divmod(train_samples, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "\n",
    "# Set up a data stream for the training set\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(train_samples)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "\n",
    "batches = data_stream()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the LeNet-300-100 architecture is used to train a neural network to classify handwritten digits.\n",
    "This is a feed-forward neural network with 2 hidden layers, each with 300 and 100 neurons respectively.\n",
    "The ReLU activation function is used for the hidden layers, and the log softmax activation function is used for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    model = hk.Sequential(\n",
    "        [\n",
    "            hk.Linear(300),  # 300 hidden units\n",
    "            jax.nn.relu,\n",
    "            hk.Linear(100),  # 100 hidden units\n",
    "            jax.nn.relu,\n",
    "            hk.Linear(10),  # 10 output units\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "    )\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "# Trasform into pure functions and remove rng (as it is unnecessary)\n",
    "network = hk.without_apply_rng(hk.transform(forward_pass))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is standard for classification tasks, we will use a softmax output layer to predict probabilities for each of our target classes, and we will use the cross entropy loss to measure the network's performance. \n",
    "A function for computing the accuracy of the model is also defined here, which we will use in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(params, x, y):\n",
    "    y_pred = network.apply(params, x)\n",
    "    return -jnp.mean(jnp.sum(y_pred * y, axis=-1))\n",
    "\n",
    "\n",
    "def accuracy(params, x, y):\n",
    "    predictions = network.apply(params, x)\n",
    "    return jnp.mean(jnp.argmax(predictions, axis=-1) == jnp.argmax(y, axis=-1))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimise the parameters of the network we will use the ADAM optimiser from the `optax` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise adam optimiser returning the gradient transformation functions\n",
    "opt_init, opt_update = optax.adam(learning_rate)\n",
    "\n",
    "# Initialise the model's parameters and the optimiser's state\n",
    "params = network.init(next(key_seq), jnp.zeros([1, input_size]))\n",
    "opt_state = opt_init(params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the network parameters are optimised to accurately predict the labels of the training data. The network is then tested on the test data to see how well it performs on data it has not seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 in 3.69 sec\n",
      "Training set accuracy 96.86%\n",
      "Validation set accuracy 96.14%\n",
      "Epoch 1 in 2.60 sec\n",
      "Training set accuracy 97.68%\n",
      "Validation set accuracy 96.66%\n",
      "Epoch 2 in 2.53 sec\n",
      "Training set accuracy 97.51%\n",
      "Validation set accuracy 96.22%\n",
      "Epoch 3 in 2.51 sec\n",
      "Training set accuracy 98.56%\n",
      "Validation set accuracy 97.13%\n",
      "Epoch 4 in 2.49 sec\n",
      "Training set accuracy 98.73%\n",
      "Validation set accuracy 97.21%\n",
      "Epoch 5 in 2.53 sec\n",
      "Training set accuracy 98.99%\n",
      "Validation set accuracy 97.14%\n",
      "Epoch 6 in 2.51 sec\n",
      "Training set accuracy 98.45%\n",
      "Validation set accuracy 96.48%\n",
      "Epoch 7 in 2.52 sec\n",
      "Training set accuracy 98.81%\n",
      "Validation set accuracy 97.02%\n",
      "Epoch 8 in 2.54 sec\n",
      "Training set accuracy 98.91%\n",
      "Validation set accuracy 97.18%\n",
      "Epoch 9 in 2.58 sec\n",
      "Training set accuracy 98.99%\n",
      "Validation set accuracy 97.14%\n",
      "\n",
      "Starting testing...\n",
      "Test set accuracy 97.43%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Record epoch start time\n",
    "    \n",
    "    # Iterate over the training batches and optimise the parameters\n",
    "    for _ in range(num_batches):\n",
    "        x, y = next(batches)\n",
    "\n",
    "        # Calculate the loss and gradients\n",
    "        loss, grad = jax.value_and_grad(cross_entropy_loss)(params, x, y)\n",
    "        \n",
    "        # Update the parameters and optimiser state\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "    epoch_time = time.time() - start_time  # Record epoch end time\n",
    "\n",
    "    # calculate training and test accuracy\n",
    "    train_acc = accuracy(params, train_images, train_labels) * 100\n",
    "    val_acc = accuracy(params, val_images, val_labels) * 100\n",
    "\n",
    "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
    "    print(f\"Training set accuracy {train_acc:0.2f}%\")\n",
    "    print(f\"Validation set accuracy {val_acc:0.2f}%\")\n",
    "\n",
    "print(\"\\nStarting testing...\")\n",
    "test_acc = accuracy(params, test_images, test_labels) * 100\n",
    "print(f\"Test set accuracy {test_acc:0.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41ff81a5b4cf0953cdd65a27f8e69bd835710a822f8713c0065f69c54480bbd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
